---
title: "Making statistics work for (evolutionary language) scientists"
author: "Erik J. Ringen"
format:
  revealjs:
    incremental: false
    slide-number: true
    theme: default
    smaller: true
---

# Part 1: Marginal effects

## About me

- I earned my PhD in Evolutionary Anthropology <span class="fragment strike">during the 447th lunation of the third millennium, when Mars was at an orbital longitude of ~187 degrees</span><span class="fragment"> in May 2023</span>

- To get from Zurich to here, I travelled <span class="fragment strike">807 light-microseconds</span><span class="fragment"> 242 kilometers</span>

## The way we report statistics is often confusing (and misleading)

![](figures/confusing_stats.png)

## Regression coefficient table figure here

## Descriptive statistics and data viz

- In contrast, descriptive statistics and data visualization are usually presented in a way that is easy to understand.
- Why? They present simple contrasts with intuitive units.
- What if we could do the same for statistical models?

## Marginal effects
- We can do better. Use the model to calculate *marginal effects* that are easy to interpret and communicate.
    - marginal effects are unit-specific contrasts, i.e., between two groups or between values of a continuous predictor.
    - The `marginaleffects` package makes these calculations easy for a wide range of models in both R and Python.

## How can statistical models help us answer research questions?

- Quantitative research relies on models with many moving parts (i.e., parameters).

- In all but the simplest models, no single parameter can answer our research question. Direct interpretation of model coefficients is unintuitive at best, misleading at worst.

- We can do better. Use the model to calculate *marginal effects* that are easy to interpret and communicate.
    - marginal effects are unit-specific contrasts, i.e., between two groups or between values of a continuous predictor.
    - The `marginaleffects` package makes these calculations easy for a wide range of models in both R and Python.

## In simple models, a parameter might be all we need

:::: {.columns}
::: {.column width="50%"}
- Example: do bilingual children have greater cognitive flexibility than monolingual children?
    - Data: 200 children from some population
    - Estimand: what is the average difference in cognitive flexibility (measured by a psychometric instrument ranging from 0 to 100) between bilingual and monolingual children?
:::

::: {.column width="50%"}
```{r, echo=FALSE}
library(tidyverse)

N <- 200

bilingual <- sample(c(0, 1), N, replace = TRUE)
cognitive_flexibility <- rnorm(N, bilingual*0.45, 1)
cognitive_flexibility  <- pnorm(cognitive_flexibility) * 100

d <- data.frame(bilingual = ifelse(bilingual == 0, "monolingual", "bilingual"), cognitive_flexibility = cognitive_flexibility)

ggplot(d, aes(x = bilingual, y = cognitive_flexibility, color = bilingual)) +
    geom_jitter(alpha = 0.7, width = 0.05) +
    geom_boxplot(alpha = 0.1) +
    theme_classic(base_size = 24) +
    theme(legend.position = "none") +
    labs(x = "", y = "Cognitive flexibility")
```

```{r}
lm(cognitive_flexibility ~ bilingual)
```
:::

::::

# Formally defining the estimand

- For a given child, let $Y_i(\text{bilingual})$ be the outcome if the child is bilingual and $Y_i(\text{monolingual})$ be the outcome if the child is monolingual. The estimand is the average difference in potential outcomes: $\frac{1}{N} \sum_{i=1}^N [Y_i(\text{bilingual}) - Y_i(\text{monolingual})]$.

- In other words, we imagine that we could randomly assign each child to be bilingual or monolingual, and then observe the difference in their individual outcomes. So, how does this connect to our model?

- Recall our simple linear model: $Y_i = \alpha + \beta \text{bilingual}_i + \epsilon_i$

- For each child $i$, the expected value of the outcome if the child is bilingual is $Y_i(\text{bilingual})$ = $\alpha + \beta$.
- The expected value of the outcome if the child is monolingual is $Y_i(\text{monolingual})$ = $\alpha$.
- Therefore the average difference in potential outcomes is simply ($\alpha + \beta$) - $\alpha$ = $\beta$.
    - It will almost never be this simple in real research contexts.

# Interactions

![](figures/hall_of_mirrors.png)

# Adding complexity: interactions

- What if the effect of bilingualism depends on the child's age (an interaction term)?
- Formula: $Y_i = \alpha + \beta \text{bilingual}_i + \gamma \text{age}_i + \delta \text{bilingual}_i \times \text{age}_i + \epsilon_i$
- For each child $i$, the expected value of the outcome if the child is bilingual is $Y_i(\text{bilingual})$ = $\alpha + \beta + \gamma \text{age}_i + \delta \text{age}_i$.
- The expected value of the outcome if the child is monolingual is $Y_i(\text{monolingual})$ = $\alpha + \gamma \text{age}_i$.
- The estimand is the average difference in potential outcomes: $\frac{1}{N} \sum_{i=1}^N [Y_i(\text{bilingual}) - Y_i(\text{monolingual})] = \frac{1}{N} \sum_{i=1}^N [(\alpha + \beta + \gamma \text{age}_i + \delta \text{age}_i) - (\alpha + \gamma \text{age}_i)] = \frac{1}{N} \sum_{i=1}^N (\beta + \delta \text{age}_i) = \beta + \delta \bar{\text{age}}$.
- Thus, the marginal effect is no longer reducible to a single parameter.

# Once we start using generalized linear models, everything interacts!


# Many types of marginal effects

- Average marginal effect
- Marginal effect at the mean
- Conditional marginal effect
- Average slope
- Takeaway: statisticians are terrible at naming things.

---

![Three critical choices in quantitative research. From: Lundberg, I., Johnson, R., & Stewart, B. M. (2021). What is your estimand? Defining the target quantity connects statistical evidence to theory. American Sociological Review, 86(3), 532-565.](figures/estimand_flow.png)

---

![Three critical choices in quantitative research. From: Lundberg, I., Johnson, R., & Stewart, B. M. (2021). What is your estimand? Defining the target quantity connects statistical evidence to theory. American Sociological Review, 86(3), 532-565.](figures/estimand_flow_highlight.png)

### A regression is a machine with many moving parts

# Break/Q&A sesion

# Part 2: Work-flow, code review. Fear is the mind killer

### Kim-Jung Gi drawing gif here. Disconnect between what we see in published reserach and how statistical modelling works in practice.

Statistics can feel like a thankless list of things that all have to be right or else *nothing* is right.
- Not totally wrong, but leads to anxiety

### Slow is smooth, smooth is fast

### Fit fast, fail fast

- Recall that a regression is a machine with many moving parts. Just as we can't understand the joint behavior from the sum of the parts, we often cannot understand a model's failures from the individual parts.

- Need to simply the model until it works, then gradually add back complexity to diagonose what went wrong.

- Folk theorem of statistical computing: 
Problems in statistical computing are often caused by problems in the model/data.
