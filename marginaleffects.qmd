---
title: "Making statistics work for (evolutionary language) scientists"
author: "Erik J. Ringen"
format:
  revealjs:
    auto-stretch: false
    incremental: true
    slide-number: true
    theme: default
    smaller: true
---

## Introduction

This session is about how statistics can be in service of science.
- That point might sound mundane, but I think it is actually profound.

- Key ideas:
    - Scientist have to learn the language of statistical models
    - But a lot gets lost in translation, particularly when we try to translate back to substantive scientific questions from the output of statistical models.

- Goals:
    - Teach you a very general way to translate statistical models back into substantive scientific questions.
    - 

# Part 1: Marginal effects

## About me

- I earned my PhD in Evolutionary Anthropology <span class="fragment strike">during the 447th lunation of the third millennium, when Mars was at an orbital longitude of ~187 degrees</span><span class="fragment"> in May 2023</span>

- To get from Zurich to here, I travelled <span class="fragment strike">807 light-microseconds</span><span class="fragment"> 242 kilometers</span>

## The way we report statistics is often confusing (and misleading)

![](figures/confusing_stats.png)

## The way we report statistics is often confusing (and misleading)

![](figures/regression_table.png)

---

### In contrast, descriptive statistics and data viz are easy to understand

```{r}
set.seed(123)
library(tidyverse)
N <- 250

multilingual <- sample(c(0, 1), N, replace = TRUE)

age <- runif(N, 7, 12)
age_s <- (age - mean(age)) / sd(age)
age_c <- age - mean(age)
cognitive_flexibility <- rnorm(N, multilingual*0.3 + age_s*multilingual*0.2 + age_s*0.1, 0.5)
cognitive_flexibility  <- pnorm(cognitive_flexibility) * 100

d <- data.frame(multilingual = ifelse(multilingual == 0, "monolingual", "multilingual"), cognitive_flexibility = cognitive_flexibility, age = age, age_c = age_c)

ggplot(d, aes(x = multilingual, y = cognitive_flexibility, color = multilingual)) +
    geom_jitter(alpha = 0.7, width = 0.05) +
    geom_boxplot(alpha = 0.1) +
    theme_classic(base_size = 24) +
    theme(legend.position = "none") +
    labs(x = "", y = "Cognitive flexibility score")
```

'`r paste0("Monolingual participants have a mean cognitive flexibility score of ", 
           round(mean(d$cognitive_flexibility[d$multilingual=="monolingual"]), 2),
           " (SD = ", 
           round(sd(d$cognitive_flexibility[d$multilingual=="monolingual"]), 2),
           "), while multilingual participants have a mean score of ", 
           round(mean(d$cognitive_flexibility[d$multilingual=="multilingual"]), 2),
           " (SD = ",
           round(sd(d$cognitive_flexibility[d$multilingual=="multilingual"]), 2),
           ").")`'

- Simple comparisons with scientifically-meaningful units.
- What if we could do the same for statistical models?

## Predictions, not parameters
- Main idea: rather than directly interpret parameters, use the model to make predictions
    - contrasts between different predictions consitute *marginal effects*
- The `marginaleffects` package by Vincent Arel-Bundock makes these calculations easy for a wide range of models in both R and Python.

## In simple models, a parameter might be all we need

:::: {.columns}
::: {.column width="60%"}
- Example: do multilingual children have greater cognitive flexibility than monolingual children?
    - *Data*: 200 children from some population
    - *Estimand*: what is the average difference in cognitive flexibility (measured by a psychometric instrument ranging from 0 to 100) between bilingual and monolingual children?
    - *Estimate*: 'Multilingual participants have an average cognitive flexibility score of 11.908 [5.386, 18.430] points higher than monolingual participants.'
:::

::: {.column width="40%"}
::: {.fragment}
```{r, echo=FALSE}
ggplot(d, aes(x = multilingual, y = cognitive_flexibility, color = multilingual)) +
    geom_jitter(alpha = 0.7, width = 0.05) +
    geom_boxplot(alpha = 0.1) +
    theme_classic(base_size = 24) +
    theme(legend.position = "none") +
    labs(x = "", y = "Cognitive flexibility")
```
:::

::: {.fragment}
```{r}
summary(lm(cognitive_flexibility ~ multilingual))
```
:::
:::
::::

## A regression is a machine that makes predictions

$$ \text{Cognitive flexibility} = \alpha + \beta \text{Multilingual} + \epsilon $$
$$ \epsilon \sim \text{Normal}(0, \sigma^2) $$

- The parts of the machine are the parameters ($\alpha, \beta, \sigma$):
  - To make a prediction for a monolingual child, just take the intercept ($\alpha$).
  - To make a prediction for a multilingual child, add the coefficient for multilingual ($\beta$) to the intercept.
- Linear regression is such a simple machine that it offers a 1:1 mapping between the parameters and the estimand (average difference between bilingual and monolingual children).

## Interactions imply heterogeneity

:::: {.columns}

::: {.column width="60%"}
- If we add an interaction term (moderator) to our model, the estimand is no longer reducible to a single parameter.
- To see why, let's formalize the estimand:
  - For a given child, let $Y_i(\text{multilingual})$ be the outcome if the child is bilingual and $Y_i(\text{monolingual})$ be the outcome if the child is monolingual. 
  - The estimand is the average difference in potential outcomes: $\frac{1}{N} \sum_{i=1}^N [Y_i(\text{multilingual}) - Y_i(\text{monolingual})]$. Known as the *average treatment effect* (ATE), or average marginal effect (AME).
  - In our simple linear regression, all children are identical, conditional on their mono/multilingual status, which allows us to reduce the estimand to a single parameter.
:::

::: {.column width="40%"}
::: {.fragment}
![](figures/ATE.png){width="75%"}
:::
:::

::::

## Interactions imply heterogeneity

What if age moderates the effect of multilingualism?

:::: {.columns}
::: {.column width="60%"}
```{r}
m_centered <- lm(cognitive_flexibility ~ multilingual * age_c)

summary(m_centered)
```
:::

::: {.column width="40%"}
```{r}
ggplot(d, aes(x = age_c, y = cognitive_flexibility, color = multilingual)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", se = FALSE) +
    theme_classic(base_size = 28) +
    theme(legend.title = element_blank()) +
    scale_color_manual(values = c("monolingual" = "royalblue", 
                                "multilingual" = "darkorange")) +
    labs(x = "Age (centered)", y = "Cognitive flexibility")
```
:::
::::

- Which coefficient corresponds to the estimand (average marginal effect)?
    - None! The AME depends on the distribution of the moderator (age):
        - The magnitude of the multilingual effect depends on age--now it matters which children we are averaging over.

## Meaning of interaction term is highly contextual

On the previous slide, the interaction term was mean-centered. What if we use the raw age variable instead?

:::: {.columns}
::: {.column width="60%"}
```{r}
m_uncentered <- lm(cognitive_flexibility ~ multilingual * age)

summary(m_uncentered)
```
:::

::: {.column width="40%"}
```{r}
ggplot(d, aes(x = age, y = cognitive_flexibility, color = multilingual)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = "lm", se = FALSE) +
    theme_classic(base_size = 28) +
    theme(legend.title = element_blank()) +
    scale_color_manual(values = c("monolingual" = "royalblue", 
                                "multilingual" = "darkorange")) +
    labs(x = "Age", y = "Cognitive flexibility")
```
:::
::::

- The data visualization looks the same, but the model seems to be telling us something different.
    - In fact, the effect we are seeking is 'lost in translation'.
    - By generating predictions from the model, these contradictions will melt away.

## Getting started with `marginaleffects`: predictions

```{r, echo = T}
library(marginaleffects)

uncentered_model <- lm(cognitive_flexibility ~ multilingual * age, data = d)

preds <- predictions(uncentered_model)

preds |> 
    as.data.frame() |> 
    select(rowid, multilingual, age, estimate, conf.low, conf.high) |> 
    print(digits = 2)
```

## Getting started with `marginaleffects`: predictions

```{r, echo = T}
library(marginaleffects)

centered_model <- lm(cognitive_flexibility ~ multilingual * age_c, data = d)

preds <- predictions(centered_model)

preds |> 
    as.data.frame() |> 
    select(rowid, multilingual, age_c, estimate, conf.low, conf.high) |> 
    print(digits = 2)
```

## From predictions to comparisons

```{r, echo = T}
comparisons <- comparisons(uncentered_model, variables = "multilingual")

comparisons |> 
    as.data.frame() |> 
    select(rowid, contrast, age, estimate, conf.low, conf.high) |> 
    print(digits = 2)
```

## Calcualting average marginal effects

```{r, echo = T}
AME <- avg_comparisons(uncentered_model)

AME |> 
    as.data.frame() |> 
    select(term, contrast, estimate, conf.low, conf.high) |> 
    print(digits = 2)
```

## marginaleffects can also make plots

```{r, echo = T}
plot_predictions(uncentered_model, condition = list("multilingual"))
```

## marginaleffects can also make plots

```{r, echo = T}
plot_predictions(uncentered_model, condition = list("age"))
```

## marginaleffects can also make plots

```{r, echo = T}
plot_predictions(uncentered_model, condition = list("age", "multilingual"))
```

## marginaleffects can also make plots

- Because the plots are made with `ggplot2`, we can customize them as we like.

```{r, echo = T}
plot_predictions(uncentered_model, condition = list("age", "multilingual")) +
    theme_classic(base_size = 24) +
    theme(legend.title = element_blank()) +
    scale_color_manual(values = c("monolingual" = "royalblue", 
                                "multilingual" = "darkorange")) +
    scale_fill_manual(values = c("monolingual" = "royalblue", 
                                "multilingual" = "darkorange")) +
    labs(x = "Age", y = "Cognitive flexibility score")
```

## Non-linear predictors

- What if we want to contrast the effect of bilingualism at different levels of age?
- Quadratic terms:

$$ \text{Cognitive flexibility} = \alpha + \beta \text{Multilingual} + \beta \text{Age} + \beta \text{Multilingual} \times \text{Age} + \epsilon $$

## Generalized linear models

- non-collapsiblity of odds ratios
- sign-reversals on the log scale!

## Marginal effects in practice

- average marginal effect (aka average treatment effect)

## Conditional effects

- we may wish to calculate the effect of multilingualism across different strata (i.e., age)
- rather than leave age as their original values, we hold them constantle')

## Non-linearity, continous treatment effects

- average slopes!
- given a very small change in the predictor, what is the change in the outcome?

### Interpreting regression models is a full of traps that can substantivey change our conclusions. Working with predictions and marginal effects allows us to avoid many of these traps.

```{r}
library(marginaleffects)

avg_comparisons(m_centered)
avg_comparisons(m_uncentered)
```

## Can marginal effects improve the reliability of science?

## Red Card Dataset (many analysts)

- Football players who were red-carded for conduct in the 2018-2019 season.
- 100+ analysts, 100+ estimates (check this!)

## Many analysts, many estimates

![](figures/many_analysts1.png)

## But what is the estimand?

![](figures/many_analysts2.png)


----

### So we start out advocating for unit-specific contrasts, but then we realize that we need to make a choice about what we want to contrast. So need to define the estimand, and the connection between theory, estimand, and estimator.

### If we continue to interpret the model parameters as if it was a simple linear regression, then we are letting the model dictate the estimand. In this sense, the mechanistic details of the statistical model are changing the estimand, and therefore the scientific question.

## Problems with parameters

- Quantitative research relies on models with many moving parts (i.e., parameters).

- In all but the simplest models, no single parameter can answer our research question. Direct interpretation of model coefficients is unintuitive at best, misleading at worst.

- We can do better. Use the model to calculate *marginal effects* that are easy to interpret and communicate.
    - marginal effects are unit-specific contrasts, i.e., between two groups or between values of a continuous predictor.
    - The `marginaleffects` package makes these calculations easy for a wide range of models in both R and Python.

# Formally defining the estimand

- For a given child, let $Y_i(\text{bilingual})$ be the outcome if the child is bilingual and $Y_i(\text{monolingual})$ be the outcome if the child is monolingual. The estimand is the average difference in potential outcomes: $\frac{1}{N} \sum_{i=1}^N [Y_i(\text{bilingual}) - Y_i(\text{monolingual})]$.

- In other words, we imagine that we could randomly assign each child to be bilingual or monolingual, and then observe the difference in their individual outcomes. So, how does this connect to our model?

- Recall our simple linear model: $Y_i = \alpha + \beta \text{bilingual}_i + \epsilon_i$

- For each child $i$, the expected value of the outcome if the child is bilingual is $Y_i(\text{bilingual})$ = $\alpha + \beta$.
- The expected value of the outcome if the child is monolingual is $Y_i(\text{monolingual})$ = $\alpha$.
- Therefore the average difference in potential outcomes is simply ($\alpha + \beta$) - $\alpha$ = $\beta$.
    - It will almost never be this simple in real research contexts.

# Why we need to define the estimand
- Redcard dataset: diferences between analysts dissipates once we clearly define the estimand.

# Interactions

![](figures/hall_of_mirrors.png)

# Adding complexity: interactions

- What if the effect of bilingualism depends on the child's age (an interaction term)?
- Formula: $Y_i = \alpha + \beta \text{bilingual}_i + \gamma \text{age}_i + \delta \text{bilingual}_i \times \text{age}_i + \epsilon_i$
- For each child $i$, the expected value of the outcome if the child is bilingual is $Y_i(\text{bilingual})$ = $\alpha + \beta + \gamma \text{age}_i + \delta \text{age}_i$.
- The expected value of the outcome if the child is monolingual is $Y_i(\text{monolingual})$ = $\alpha + \gamma \text{age}_i$.
- The estimand is the average difference in potential outcomes: $\frac{1}{N} \sum_{i=1}^N [Y_i(\text{bilingual}) - Y_i(\text{monolingual})] = \frac{1}{N} \sum_{i=1}^N [(\alpha + \beta + \gamma \text{age}_i + \delta \text{age}_i) - (\alpha + \gamma \text{age}_i)] = \frac{1}{N} \sum_{i=1}^N (\beta + \delta \text{age}_i) = \beta + \delta \bar{\text{age}}$.
- Thus, the marginal effect is no longer reducible to a single parameter.

# Once we start using generalized linear models, everything interacts!


# Many types of marginal effects

- Average marginal effect
- Marginal effect at the mean
- Conditional marginal effect
- Average slope
- Takeaway: statisticians are terrible at naming things.

---

![Three critical choices in quantitative research. From: Lundberg, I., Johnson, R., & Stewart, B. M. (2021). What is your estimand? Defining the target quantity connects statistical evidence to theory. American Sociological Review, 86(3), 532-565.](figures/estimand_flow.png)

---

![Three critical choices in quantitative research. From: Lundberg, I., Johnson, R., & Stewart, B. M. (2021). What is your estimand? Defining the target quantity connects statistical evidence to theory. American Sociological Review, 86(3), 532-565.](figures/estimand_flow_highlight.png)

### A regression is a machine with many moving parts

## Hands on session

- 

# Break/Q&A sesion

# Part 2: Work-flow, code review. Fear is the mind killer

This section is about computation, but it is also about anxiety and the fear of making mistakes.

## Truth: everyone has bugs in their code

### Kim-Jung Gi drawing gif here. Disconnect between what we see in published reserach and how statistical modelling works in practice.

Statistics can feel like a thankless list of things that all have to be right or else *nothing* is right.
- Not totally wrong, but leads to anxiety

## Code review anxiety

- 7 years ago, I had published my first, first-authored paper. The data and code were posted on GitHub.

- A few days after the paper went live, I got a notification for a pull request on my repo: Bret Beheim had found a bug in my code.

- In the end, it was inconsequential. But it could have been a disaster.

### Slow is smooth, smooth is fast

### Fit fast, fail fast

- Recall that a regression is a machine with many moving parts. Just as we can't understand the joint behavior from the sum of the parts, we often cannot understand a model's failures from the individual parts.

- Need to simply the model until it works, then gradually add back complexity to diagonose what went wrong.

- Folk theorem of statistical computing: 
Problems in statistical computing are often caused by problems in the model/data.

## Multiverse of madness

- Specification curves
- Key idea: figure out sensitivity of estimates to reasonable range of model specifications, and which qualitative insights are robust to these choices.

:::: {.columns}

::: {.column width="50%"}
```{r}
# Model with standardized age
summary(lm(cognitive_flexibility ~ multilingual * age_s))
```
:::

::: {.column width="50%"}
```{r}
# Model with raw age
summary(lm(cognitive_flexibility ~ multilingual * age))
```
:::
::::

:::
